{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9TGJ8Tr4IBN"
   },
   "source": [
    "# 1. Importación de librerías y carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4hdwdC16JNZz"
   },
   "outputs": [],
   "source": [
    "SCALED=False\n",
    "STANDARIZED=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GIG5_asbnMfv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "\n",
    "import haversine as hs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers import LSTM, GRU, Dense, Reshape, Dropout, Bidirectional, \\\n",
    "  RepeatVector, TimeDistributed, Conv1D, MaxPooling1D, Flatten, ConvLSTM2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from haversine import inverse_haversine, Direction\n",
    "from attention import Attention\n",
    "\n",
    "if SCALED:\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "elif STANDARIZED:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from shapely import geometry\n",
    "import geopy\n",
    "import geopy.distance\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RSGMAKCXP5Tw"
   },
   "outputs": [],
   "source": [
    "# Hacemos el resultado reproducible\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "# Declaramos el tipo de float, en este caso float64\n",
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "T-eyu4JlxZdK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandr\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time(s)</th>\n",
       "      <th>vehicle_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     time(s)  vehicle_id\n",
       "0        1.0           0\n",
       "31       2.0           0\n",
       "64       3.0           0\n",
       "100      4.0           0\n",
       "158      5.0           0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leemos y guardamos la info de segudo y vehículo\n",
    "df = pd.read_csv('positions.csv')\n",
    "\n",
    "df_data = df.sort_values(by=['vehicle_id', 'time(s)'])\n",
    "\n",
    "columns = ['time(s)', 'vehicle_id']\n",
    "df = df_data[columns]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wffnrEC1mANF"
   },
   "outputs": [],
   "source": [
    "# Load MinMaxScaler for target variables\n",
    "if SCALED:\n",
    "    MINMAXSCALER_FILE = './minmaxscaler.pkl'#os.path.join(\"TFM\", \"implementation\",\n",
    "                             #\"data_preparation\", \"minmaxscaler.pkl\")\n",
    "    with open(MINMAXSCALER_FILE, 'rb') as f_minmaxscaler:\n",
    "        minmaxscaler = pickle.load(f_minmaxscaler)\n",
    "if STANDARIZED:\n",
    "    TARGETS_STAND_FILE = './' #os.path.join(\"TFM\", \"implementation\",\n",
    "                                     #\"data_preparation\", \"targets_stand.pkl\")\n",
    "    with open(TARGETS_STAND_FILE, 'rb') as f_scaler:\n",
    "        standscaler_targets = pickle.load(f_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dSL4UPD5RgQk",
    "outputId": "90e8bba5-1773-49cd-a3db-1a027103afe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (6005877, 20, 5)  y_train: (6005877, 5, 5)\n",
      "X_val: (2573948, 20, 5)  y_val: (2573948, 5, 5)\n",
      "X_test: (2402, 20, 5)  y_test: (2402, 5, 5)\n",
      "Vehicle_ids: 8972\n"
     ]
    }
   ],
   "source": [
    "# Load training dataset\n",
    "if SCALED:\n",
    "    DATASET_TRAIN = os.path.join('./',\n",
    "                               \"ventanas_no_coincidentes_train_val_scaled.npz\")\n",
    "elif STANDARIZED:\n",
    "    DATASET_TRAIN = os.path.join('./',\n",
    "                               \"ventanas_no_coincidentes_train_val_stand.npz\")\n",
    "else:\n",
    "    DATASET_TRAIN = os.path.join('./',\n",
    "                               \"ventanas_no_coincidentes_train_val.npz\")\n",
    "    \n",
    "npzfile = np.load(DATASET_TRAIN)\n",
    "X_train = npzfile['X_train']\n",
    "X_val = npzfile['X_val']\n",
    "y_train = npzfile['y_train']\n",
    "y_val = npzfile['y_val']\n",
    "print(\"X_train: {}  y_train: {}\\nX_val: {}  y_val: {}\"\n",
    "      .format(X_train.shape, y_train.shape, X_val.shape, y_val.shape))\n",
    "\n",
    "# Load testing dataset\n",
    "if SCALED:\n",
    "    DATASET_TEST = os.path.join('./',\n",
    "                              \"ventanas_no_coincidentes_test_scaled.npz\")\n",
    "elif STANDARIZED:\n",
    "    DATASET_TEST = os.path.join('./',\n",
    "                              \"ventanas_no_coincidentes_test_stand.npz\")\n",
    "else:\n",
    "    DATASET_TEST = os.path.join('./',\n",
    "                              \"ventanas_no_coincidentes_test.npz\")\n",
    "npzfile = np.load(DATASET_TEST)\n",
    "X_test = npzfile['X_test']\n",
    "y_test = npzfile['y_test']\n",
    "print(\"X_test: {}  y_test: {}\".format(X_test.shape, y_test.shape))\n",
    "\n",
    "# Load testing vehicle ids list\n",
    "VEHICLE_IDS_FILE = os.path.join('./', \"vehicle_ids.pkl\")\n",
    "with open(VEHICLE_IDS_FILE, 'rb') as f_vehicle_ids:\n",
    "    vehicle_ids = pickle.load(f_vehicle_ids)\n",
    "print(\"Vehicle_ids: {}\".format(len(vehicle_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TCbub7-hjnzp",
    "outputId": "04c0c7b3-a603-4662-a30f-a46c14354071"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>length</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vehicle_id  length  width\n",
       "0           0     5.0    1.8\n",
       "1           2     5.0    1.8\n",
       "2           3     5.0    1.8\n",
       "3           4     2.2    0.9\n",
       "4           5     5.0    1.8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset with vehicles dimensions\n",
    "DATASET_DIM = os.path.join('./', \"dimensions.csv\")\n",
    "df_dim = pd.read_csv(DATASET_DIM)\n",
    "df_dim = df_dim[['vehicle_id', 'length', 'width']]\n",
    "df_dim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AnJOyIkcjv_5",
    "outputId": "04b9d2cf-e375-4385-c4e2-764952e4977c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>victim_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1240</td>\n",
       "      <td>5722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>209</td>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>863</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6444</td>\n",
       "      <td>3528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>798</td>\n",
       "      <td>2787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vehicle_id  victim_id\n",
       "0        1240       5722\n",
       "1         209        802\n",
       "2         863        300\n",
       "3        6444       3528\n",
       "4         798       2787"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset with vehicles colision\n",
    "#DATASET_COL = os.path.join('./', \"collisions_1k.csv\")\n",
    "df_col = pd.read_csv('collisions_2k.csv')\n",
    "df_col = df_col[['vehicle_id', 'victim_id']].astype(int)\n",
    "df_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12mhfvdfjzhr",
    "outputId": "ab399584-1abd-45b2-b431-4c883a873d32"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31477</td>\n",
       "      <td>34003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33063</td>\n",
       "      <td>33228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33130</td>\n",
       "      <td>33973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33268</td>\n",
       "      <td>33958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33268</td>\n",
       "      <td>34256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      v1     v2\n",
       "0  31477  34003\n",
       "1  33063  33228\n",
       "2  33130  33973\n",
       "3  33268  33958\n",
       "4  33268  34256"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset with vehicles no-colision\n",
    "#DATASET_NOCOL = os.path.join('./', \"no_collisions_1k.csv\")\n",
    "df_nocol = pd.read_csv('no_collisions_2k.csv')\n",
    "df_nocol = df_nocol[['v1', 'v2']]\n",
    "df_nocol.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2120, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_col.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2120, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nocol.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zduzl9Va4w6D"
   },
   "source": [
    "# 2. Generación y entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "AIgxF4y1QKvY"
   },
   "outputs": [],
   "source": [
    "#if SCALED:\n",
    "#    MODELS_FOLDER = os.path.join(\"TFM\", \"implementation\", \"models\", \"scaled\")\n",
    "#elif STANDARIZED:\n",
    "#    MODELS_FOLDER = os.path.join(\"TFM\", \"implementation\", \"models\", \"standarized\")\n",
    "#else:\n",
    "#    MODELS_FOLDER = os.path.join(\"TFM\", \"implementation\", \"models\", \"real\")\n",
    "    \n",
    "MODELS_FOLDER = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows parameters\n",
    "INPUT_WIDTH = 20\n",
    "OUTPUT_WIDTH = 5\n",
    "OFFSET_WIDTH = 0\n",
    "\n",
    "# Input and output columns\n",
    "INPUT_COL = ['latitude(m)', 'longitude(m)', 'heading', 'speed(m/s)', 'acceleration(m/s²)']\n",
    "OUTPUT_COL = ['latitude(m)', 'longitude(m)', 'heading', 'speed(m/s)', 'acceleration(m/s²)']\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "PATIENCE = 3\n",
    "REDUCELR = 0.2\n",
    "\n",
    "def physics_loss(y_true, y_pred):\n",
    "     # Para facilitar el cálculo, hacemos un reshape de y_true para juntar las 64 muestras (batch_size)\n",
    "    a,b,c = tf.shape(y_true)\n",
    "    y_true = tf.reshape(y_true, [a.numpy()*b.numpy(),c.numpy()])\n",
    "    y_pred = tf.reshape(y_pred, [a.numpy()*b.numpy(),c.numpy()])\n",
    "    \n",
    "    \n",
    "    # Calculamos velocidades y aceleraciones\n",
    "    vx = tf.gather(y_true,[3],axis=1) * tf.math.cos(tf.gather(y_true,[2],axis=1))\n",
    "    vy = tf.gather(y_true,[3],axis=1) * tf.math.sin(tf.gather(y_true,[2],axis=1))\n",
    "    ax = tf.gather(y_true,[4],axis=1) * tf.math.cos(tf.gather(y_true,[2],axis=1))\n",
    "    ay = tf.gather(y_true,[4],axis=1) * tf.math.sin(tf.gather(y_true,[2],axis=1))\n",
    "\n",
    "    # Aplicamos las ecuaciones del movimiento para obtener la distancia recorrida en metros\n",
    "    dist_lat_m = vy + ay/2\n",
    "    dist_lon_m = vx + ax/2\n",
    "\n",
    "    # Pasamos la distancia recorrida en metros a las distancias en latitud longitud\n",
    "    dist_lat = (dist_lat_m/6371000)*(180/math.pi)\n",
    "    dist_lon = (dist_lon_m/6371000)*(180/math.pi) / tf.math.cos(tf.gather(y_true,[0],axis=1) * math.pi/180)\n",
    "\n",
    "    # Calculamos las nuevas latitud y longitud\n",
    "    pred_latitude = tf.gather(y_true,[0],axis=1) + dist_lat\n",
    "    pred_longitude = tf.gather(y_true,[1],axis=1) + dist_lon\n",
    "    \n",
    "    # La pérdida será reducir la distancia entre las predichas y las calculadas físicamente\n",
    "    loss = tf.reduce_mean(tf.square(pred_latitude -tf.gather(y_pred,[0],axis=1)) \n",
    "                          + tf.square(pred_longitude -tf.gather(y_pred,[1],axis=1)))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def compile_and_fit(model, X_train, y_train, X_val, y_val, folder, epochs=20,\n",
    "                    batch_size=32, patience=2, learning_rate=0.001, reduce_lr=0):\n",
    "    tracker = EmissionsTracker(log_level=\"error\")\n",
    "    callback_list = []\n",
    "\n",
    "    # Return best model that minimize loss function\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', patience=patience*2, mode='min',\n",
    "        restore_best_weights=True)\n",
    "    callback_list.append(early_stopping)\n",
    "\n",
    "    # Save best model that minimize loss function\n",
    "    checkpoint_file = os.path.join(folder, 'model_saved.hdf5')\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_file, monitor='val_loss', mode=\"min\",\n",
    "        save_best_only=True, verbose=0)\n",
    "    callback_list.append(checkpoint)\n",
    "  \n",
    "    if reduce_lr != 0:\n",
    "        # Reduce learning rate to minimize loss function\n",
    "        reduce_learningrate = ReduceLROnPlateau(\n",
    "            monitor='val_loss', patience=patience, mode='min', factor=reduce_lr)\n",
    "        callback_list.append(reduce_learningrate)\n",
    "\n",
    "    model.compile(loss=physics_loss,\n",
    "                  optimizer=Adam(learning_rate=learning_rate),\n",
    "                  metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    tracker.start()\n",
    "    start_time = time.time()\n",
    "    print('X_train.shape:', X_train.shape)\n",
    "    print('y_train.shape:', y_train.shape)\n",
    "\n",
    "    # Define state transition function for UKF\n",
    "    def fx(x, dt):\n",
    "        # Implement the state transition function\n",
    "        # Assuming a simple constant velocity model for demonstration\n",
    "        # x: State vector [latitude, longitude, heading, speed, acceleration]\n",
    "        # dt: Time step\n",
    "\n",
    "        # Extracting state variables\n",
    "        latitude = x[0]\n",
    "        longitude = x[1]\n",
    "        heading = x[2]\n",
    "        speed = x[3]\n",
    "        acceleration = x[4]\n",
    "\n",
    "        # Update the state based on a constant velocity model\n",
    "        new_latitude = latitude + speed * dt * math.cos(heading)\n",
    "        new_longitude = longitude + speed * dt * math.sin(heading)\n",
    "        new_heading = heading  # Assuming heading remains constant over time\n",
    "        new_speed = speed + acceleration * dt\n",
    "        new_acceleration = acceleration  # Assuming acceleration remains constant over time\n",
    "\n",
    "        # Return the updated state vector\n",
    "        return np.array([new_latitude, new_longitude, new_heading, new_speed, new_acceleration])\n",
    "\n",
    "    # Define observation function for UKF\n",
    "    def hx(x):\n",
    "        # Implement the observation function\n",
    "        # Directly observe all state variables\n",
    "        return x\n",
    "\n",
    "    # Training loop with UKF correction\n",
    "    X_corrected = []\n",
    "    for i in range(len(X_train)):\n",
    "        x = X_train[i]\n",
    "        z = y_train[i]\n",
    "\n",
    "        # Configure UKF\n",
    "        points = MerweScaledSigmaPoints(n=5, alpha=0.1, beta=2., kappa=-1)\n",
    "        ukf = UnscentedKalmanFilter(dim_x=5, dim_z=5, dt=1, fx=fx, hx=hx, points=points)\n",
    "\n",
    "        # Prediction step\n",
    "        ukf.predict()\n",
    "\n",
    "        # Update step\n",
    "        ukf.update(z)\n",
    "\n",
    "        # Corrected state estimate\n",
    "        X_corrected.append(ukf.x)\n",
    "\n",
    "    # Model training using corrected state estimate\n",
    "    print(X_train.shape)\n",
    "    print(np.array(X_corrected).shape)\n",
    "    #history = model.train_on_batch(X_train[:10], np.array(X_corrected))\n",
    "\n",
    "    history = model.fit(X_train, np.array(X_corrected), epochs=epochs, batch_size=batch_size,\n",
    "                      validation_data=(X_val, y_val),\n",
    "                      callbacks=callback_list)    \n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    emissions = tracker.stop()\n",
    "\n",
    "    # Save fit history\n",
    "    history_pickle = os.path.join(folder, 'history.pkl')\n",
    "    pickle.dump(history.history, open(history_pickle, 'wb'))\n",
    "\n",
    "    # Print and write fit statistics\n",
    "    min_val_loss = min(history.history['val_loss'])\n",
    "    best_model_index = history.history['val_loss'].index(min_val_loss)\n",
    "    print(\"\\nTraining Loss (MSE): {}\".format(history.history['loss'][best_model_index]))\n",
    "    print(\"Validation Loss (MSE): {}\".format(history.history['val_loss'][best_model_index]))\n",
    "    print(\"Training MAE: {}\".format(history.history['mean_absolute_error'][best_model_index]))\n",
    "    print(\"Validation MAE: {}\".format(history.history['val_mean_absolute_error'][best_model_index]))\n",
    "    print(\"Execution time (s): {}\".format(total_time))\n",
    "    print(\"Emissions (kg): {}\\n\".format(emissions))\n",
    "    stats_file = os.path.join(folder, \"stats.txt\")\n",
    "    with open(stats_file, 'w') as f_stats:\n",
    "        f_stats.write(\"Training Loss (MSE): {}\\n\".format(history.history['loss'][best_model_index]))\n",
    "        f_stats.write(\"Validation Loss (MSE): {}\\n\".format(history.history['val_loss'][best_model_index]))\n",
    "        f_stats.write(\"Training MAE: {}\\n\".format(history.history['mean_absolute_error'][best_model_index]))\n",
    "        f_stats.write(\"Validation MAE: {}\\n\".format(history.history['val_mean_absolute_error'][best_model_index]))\n",
    "        f_stats.write(\"Execution time (s): {}\\n\".format(total_time))\n",
    "        f_stats.write(\"Emissions (kg): {}\\n\".format(emissions))\n",
    "\n",
    "    return history\n",
    "\n",
    "def plot_history(history, start=None, end=None):\n",
    "    train_loss = history['loss'][start:end]\n",
    "    val_loss = history['val_loss'][start:end]\n",
    "    train_mae = history['mean_absolute_error'][start:end]\n",
    "    test_mae = history['val_mean_absolute_error'][start:end]\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    ax[0].plot(train_loss, label=\"Train\")\n",
    "    ax[0].plot(val_loss, label=\"Validation\")\n",
    "    ax[0].set_title(\"Training and Validation loss\")\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].set_ylabel(\"Loss\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(train_mae, label=\"Train\")\n",
    "    ax[1].plot(test_mae, label=\"Validation\")\n",
    "    ax[1].set_title(\"Training and Validation MAE\")\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    ax[1].set_ylabel(\"Mean Absolute Error\")\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def model_predictions(model, X_test, y_test):\n",
    "    def calculate_haversine(coord1, coord2):\n",
    "        return hs.haversine(coord1, coord2, unit=hs.Unit.METERS)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    if SCALED:\n",
    "        y_pred = minmaxscaler.inverse_transform(y_pred.reshape(-1, 2))\n",
    "    elif STANDARIZED:\n",
    "        y_pred = standscaler_targets.inverse_transform(y_pred.reshape(-1, 2))\n",
    "    y_pred = y_pred.reshape(-1, 4)\n",
    "\n",
    "    if SCALED:\n",
    "        y_test = minmaxscaler.inverse_transform(y_test.reshape(-1, 2))\n",
    "    elif STANDARIZED:\n",
    "        y_test = standscaler_targets.inverse_transform(y_test.reshape(-1, 2))\n",
    "    y_test = y_test.reshape(-1, 5)\n",
    "\n",
    "    df_y_pred = pd.DataFrame(\n",
    "        y_pred, columns=['latitude(pred)', 'longitude(pred)', 'heading(pred)', 'speed(m/s)(pred)'])\n",
    "    df_y_test = pd.DataFrame(y_test, columns=['latitude', 'longitude', 'heading', 'speed(m/s)', 'acceleration(m/s²)'])\n",
    "    df_pred = pd.concat([df_y_pred, df_y_test], axis=1)\n",
    "\n",
    "    distance = df_pred.apply(\n",
    "        lambda row: calculate_haversine((row['latitude(pred)'], row['longitude(pred)']),\n",
    "                                        (row['latitude'], row['longitude'])), axis=1)\n",
    "    df_pred['distance(m)'] = distance\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "id": "gEUiYzp8ZQic"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def bearing_calc(a_lat, a_lon, b_lat, b_lon): # a previous position b current position\n",
    "    rlat1 = math.radians(a_lat)\n",
    "    rlon1 = math.radians(a_lon)\n",
    "    rlat2 = math.radians(b_lat)\n",
    "    rlon2 = math.radians(b_lon)\n",
    "    dlon = math.radians(b_lon - a_lon)\n",
    "    b = math.atan2(\n",
    "      math.sin(dlon)*math.cos(rlat2),\n",
    "      math.cos(rlat1)*math.sin(rlat2)-math.sin(rlat1)*math.cos(rlat2)*math.cos(dlon))  # bearing calc\n",
    "    bd = math.degrees(b)\n",
    "    br, bn = divmod(bd + 360, 360)  # the bearing remainder and final bearing\n",
    "    return bn\n",
    "\n",
    "\n",
    "# To get a rotated rectangle at a bearing, you need to get the points of the the recatangle at that bearing\n",
    "def get_rotated_points(coordinates, bearing, width, length):\n",
    "    start = geopy.Point(coordinates)\n",
    "    width = width/1000\n",
    "    length = length/1000\n",
    "    rectlength = geopy.distance.distance(kilometers=length)\n",
    "    rectwidth = geopy.distance.distance(kilometers=width)\n",
    "    halfwidth = geopy.distance.distance(kilometers=width/2)\n",
    "    halflength = geopy.distance.distance(kilometers=length/2)\n",
    "\n",
    "    pointAB = halflength.destination(point=start, bearing=bearing)\n",
    "    pointA = halfwidth.destination(point=pointAB, bearing=0 - bearing)\n",
    "    pointB = rectwidth.destination(point=pointA, bearing=180 - bearing)\n",
    "    pointC = rectlength.destination(point=pointB, bearing=bearing - 180)\n",
    "    pointD = rectwidth.destination(point=pointC, bearing=0 - bearing)\n",
    "\n",
    "    points = []\n",
    "    for point in [pointA, pointB, pointC, pointD]:\n",
    "        coords = (point.latitude, point.longitude)\n",
    "        points.append(coords)\n",
    "\n",
    "    return points\n",
    "\n",
    "def get_colision(v1_id, v2_id, X_test_v1, X_test_v2, dimensions, model):\n",
    "    colision = 0\n",
    "\n",
    "    #y_pred_v1 = model.predict(X_test_v1.reshape(1, 20, 5), verbose=0)\n",
    "    y_pred_v1 = UKF(X_test_v1.reshape(1, 20, 5))\n",
    "    if SCALED:\n",
    "        y_pred_v1 = minmaxscaler.inverse_transform(y_pred_v1.reshape(-1, 2))\n",
    "    elif STANDARIZED:\n",
    "        y_pred_v1 = standscaler_targets.inverse_transform(y_pred_v1.reshape(-1, 2))\n",
    "    y_pred_v1 = y_pred_v1.reshape(-1, 4)\n",
    "    v1_coord_5 = y_pred_v1[-1]\n",
    "  # v1_coord_4 = y_pred_v1[-2]\n",
    "\n",
    "    v1_length = dimensions[dimensions['vehicle_id'] == v1_id].iloc[0, 1]\n",
    "    v1_width = dimensions[dimensions['vehicle_id'] == v1_id].iloc[0, 2]\n",
    "\n",
    "  # v1_bearing = bearing_calc(v1_coord_4[0], v1_coord_4[1],\n",
    "  #                           v1_coord_5[0], v1_coord_5[1])\n",
    "    v1_points = get_rotated_points(tuple(v1_coord_5[:2]), v1_coord_5[-1],\n",
    "                                 v1_width, v1_length)\n",
    "    polygon1 = geometry.Polygon(v1_points)\n",
    "\n",
    "    #y_pred_v2 = model.predict(X_test_v2.reshape(1, 20, 5), verbose=0)\n",
    "    y_pred_v2 = UKF(X_test_v2.reshape(1, 20, 5))\n",
    "    if SCALED:\n",
    "        y_pred_v2 = minmaxscaler.inverse_transform(y_pred_v2.reshape(-1, 5))\n",
    "    elif STANDARIZED:\n",
    "        y_pred_v2 = standscaler_targets.inverse_transform(y_pred_v2.reshape(-1, 2))\n",
    "    y_pred_v2 = y_pred_v2.reshape(-1, 4)\n",
    "    v2_coord_5 = y_pred_v2[-1]\n",
    "  # v2_coord_4 = y_pred_v2[-2]\n",
    "\n",
    "    v2_length = dimensions[dimensions['vehicle_id'] == v2_id].iloc[0, 1]\n",
    "    v2_width = dimensions[dimensions['vehicle_id'] == v2_id].iloc[0, 2]\n",
    "\n",
    "  # v2_bearing = bearing_calc(v2_coord_4[0], v2_coord_4[1],\n",
    "  #                           v2_coord_5[0], v2_coord_5[1])\n",
    "    v2_points = get_rotated_points(tuple(v2_coord_5[:2]), v2_coord_5[-1],\n",
    "                                 v2_width, v2_length)\n",
    "    polygon2 = geometry.Polygon(v2_points)\n",
    "\n",
    "    if polygon1.intersection(polygon2).area > 0.0:\n",
    "        colision = 1\n",
    "  \n",
    "    return colision\n",
    "\n",
    "def get_metrics(model, X_test, df, vehicle_ids, df_col, df_nocol, df_dim,\n",
    "                metrics_file):\n",
    "    total_colision = 0\n",
    "    total_no_colision = 0\n",
    "\n",
    "    true_positive = 0\n",
    "    false_negative = 0\n",
    "    false_positive = 0\n",
    "    true_negative = 0\n",
    "\n",
    "    # Vehiculos que coinciden y colisionan\n",
    "    print('coinciden y colisionan:', df_col.shape[0])\n",
    "    for i in range(df_col.shape[0]):\n",
    "        v1_id = df_col.iloc[i, 0]\n",
    "        v2_id = df_col.iloc[i, 1]\n",
    "\n",
    "        v1_ts_set = set(df[df['vehicle_id'] == v1_id].iloc[-5:, 0])\n",
    "        v2_ts_set = set(df[df['vehicle_id'] == v2_id].iloc[-5:, 0])\n",
    "        intersection = sorted(list(v1_ts_set.intersection(v2_ts_set)))\n",
    "        if len(intersection) >= 2:\n",
    "            total_colision += 1\n",
    "            v1_index = vehicle_ids.index(v1_id)\n",
    "            v2_index = vehicle_ids.index(v2_id)\n",
    "            if get_colision(v1_id, v2_id, X_test[v1_index], X_test[v2_index], df_dim, model):\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "\n",
    "  # Vehiculos que coinciden pero no colisionan\n",
    "    print('coinciden pero no colisionan:', df_nocol.shape[0])\n",
    "    for i in range(df_nocol.shape[0]):\n",
    "        v1_id = df_nocol.iloc[i, 0]\n",
    "        v2_id = df_nocol.iloc[i, 1]\n",
    "\n",
    "        v1_ts_set = set(df[df['vehicle_id'] == v1_id].iloc[-5:, 0])\n",
    "        v2_ts_set = set(df[df['vehicle_id'] == v2_id].iloc[-5:, 0])\n",
    "        intersection = sorted(list(v1_ts_set.intersection(v2_ts_set)))\n",
    "        if len(intersection) >= 2:\n",
    "            total_no_colision += 1\n",
    "            v1_index = vehicle_ids.index(v1_id)\n",
    "            v2_index = vehicle_ids.index(v2_id)\n",
    "            if get_colision(v1_id, v2_id, X_test[v1_index], X_test[v2_index], df_dim, model):\n",
    "                false_positive += 1\n",
    "            else:\n",
    "                true_negative += 1\n",
    "\n",
    "\n",
    "    accuracy = (true_positive+true_negative)/(true_positive+false_positive+false_negative+true_negative)\n",
    "    precision = true_positive/(true_positive+false_positive) if (true_positive+false_positive) != 0 else 0\n",
    "    recall = true_positive/(true_positive+false_negative) if (true_positive+false_negative) != 0 else 0\n",
    "    F1_score = 2*(recall*precision)/(recall+precision) if (recall+precision) != 0 else 0\n",
    "    specificity = true_negative/(true_negative+false_positive) if (true_negative+false_positive) != 0 else 0\n",
    "\n",
    "    with open(metrics_file, 'w') as f_metrics:\n",
    "        f_metrics.write(\"Total colision: {}\\n\".format(total_colision))\n",
    "        f_metrics.write(\"TP: {}\\n\".format(true_positive))\n",
    "        f_metrics.write(\"FN: {}\\n\".format(false_negative))\n",
    "        f_metrics.write(\"Total no colision: {}\\n\".format(total_no_colision))\n",
    "        f_metrics.write(\"FP: {}\\n\".format(false_positive))\n",
    "        f_metrics.write(\"TN: {}\\n\\n\".format(true_negative))\n",
    "        f_metrics.write(\"Accuracy: {}\\n\".format(accuracy))\n",
    "        f_metrics.write(\"Precision: {}\\n\".format(precision))\n",
    "        f_metrics.write(\"Recall: {}\\n\".format(recall))\n",
    "        f_metrics.write(\"F1 Score: {}\\n\".format(F1_score))\n",
    "        f_metrics.write(\"Specificity: {}\\n\".format(specificity))\n",
    "\n",
    "\n",
    "    print(\"Total colision: {}\".format(total_colision))\n",
    "    print(\"TP: {}\".format(true_positive))\n",
    "    print(\"FN: {}\".format(false_negative))\n",
    "    print(\"Total no colision: {}\".format(total_no_colision))\n",
    "    print(\"FP: {}\".format(false_positive))\n",
    "    print(\"TN: {}\".format(true_negative))\n",
    "    print()\n",
    "    print(\"Accuracy: {}\".format(accuracy))\n",
    "    print(\"Precision: {}\".format(precision))\n",
    "    print(\"Recall: {}\".format(recall))\n",
    "    print(\"F1 Score: {}\".format(F1_score))\n",
    "    print(\"Specificity: {}\".format(specificity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIZMo86MIBrS"
   },
   "source": [
    "## 2.1. PINNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "id": "zicDZL31AJmw"
   },
   "outputs": [],
   "source": [
    "def make_lstm_model(units, in_steps, out_steps, in_features, out_features):\n",
    "    lstm_model = Sequential()\n",
    "\n",
    "    # Shape [batch, time, in_features] => [batch, lstm_units].\n",
    "    # Adding more `lstm_units` just overfits more quickly.\n",
    "    lstm_model.add(LSTM(\n",
    "      units, input_shape=(in_steps, in_features)))\n",
    "\n",
    "    # Shape => [batch, out_steps*out_features].\n",
    "    lstm_model.add(Dense(out_steps*out_features))\n",
    "\n",
    "    # Shape => [batch, out_steps, out_features].\n",
    "    lstm_model.add(Reshape([out_steps, out_features]))\n",
    "\n",
    "    lstm_model.summary()\n",
    "\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYoL6psHIWzY"
   },
   "source": [
    "### 2.1.1. Modelo LSTM (64 unidades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svKg0PfOIK5M"
   },
   "source": [
    "#### 2.1.1.1. Creación y entrenamiento de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_86\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_86 (LSTM)              (None, 32)                4864      \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 25)                825       \n",
      "                                                                 \n",
      " reshape_86 (Reshape)        (None, 5, 5)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,689\n",
      "Trainable params: 5,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "X_train.shape: (6005877, 20, 5)\n",
      "y_train.shape: (6005877, 5, 5)\n",
      "(6005877, 20, 5)\n",
      "(10, 5, 5)\n",
      "(6005877, 20, 5)\n",
      "(10, 5, 5)\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 740.0537 - mean_absolute_error: 28.5819 - val_loss: 1734.6815 - val_mean_absolute_error: 35.1019 - lr: 0.0010\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 739.1737 - mean_absolute_error: 28.5760 - val_loss: 1733.4299 - val_mean_absolute_error: 35.0977 - lr: 0.0010\n",
      "\n",
      "Training Loss (MSE): 739.1736646699734\n",
      "Validation Loss (MSE): 1733.4298829147933\n",
      "Training MAE: 28.575967097796134\n",
      "Validation MAE: 35.09767839409035\n",
      "Execution time (s): 0.8218014240264893\n",
      "Emissions (kg): 2.092384392774084e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create model folder\n",
    "MODEL_FOLDER = os.path.join(MODELS_FOLDER, \"simple\", \"UKF_PINN_LSTM_32units\")\n",
    "os.makedirs(MODEL_FOLDER, exist_ok=True)\n",
    "\n",
    "# Model parameters\n",
    "LSTM_UNITS=32\n",
    "\n",
    "# Create, compile and fit model\n",
    "lstm_model = make_lstm_model(LSTM_UNITS, INPUT_WIDTH, OUTPUT_WIDTH,\n",
    "                             len(INPUT_COL), len(OUTPUT_COL))\n",
    "history = compile_and_fit(lstm_model, X_train, y_train, X_val, y_val,\n",
    "                          MODEL_FOLDER, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                          patience=PATIENCE, reduce_lr=REDUCELR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lYfgH92IK5N"
   },
   "source": [
    "#### 2.1.1.2. Predicción de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "eFvWA-01IK5O",
    "outputId": "e08865fe-1f5c-4ae2-aa4d-15892c168835"
   },
   "outputs": [],
   "source": [
    "df_pred = model_predictions(model, X_test, y_test, UKF(X_test))\n",
    "print(\"Distance(m): [{}, {}]\".format(min(df_pred['distance(m)']),\n",
    "                                     max(df_pred['distance(m)'])))\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "IGWWir18IK5O",
    "outputId": "cd7432cf-17c9-4963-ae12-3f52397d72b0"
   },
   "outputs": [],
   "source": [
    "df_pred.hist('distance(m)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0n_q5KHHIK5P"
   },
   "outputs": [],
   "source": [
    "n_rows = df_pred.shape[0]\n",
    "list_1seg, list_2seg, list_3seg, list_4seg, list_5seg = \\\n",
    "  [False]*n_rows, [False]*n_rows, [False]*n_rows, [False]*n_rows, [False]*n_rows\n",
    "\n",
    "for i in range(n_rows):\n",
    "    if i%5 == 0:\n",
    "        list_1seg[i] = True\n",
    "    elif i%5 == 1:\n",
    "        list_2seg[i] = True\n",
    "    elif i%5 == 2:\n",
    "        list_3seg[i] = True\n",
    "    elif i%5 == 3:\n",
    "        list_4seg[i] = True\n",
    "    elif i%5 == 4:\n",
    "        list_5seg[i] = True\n",
    "\n",
    "df_pred_1seg = df_pred.iloc[list_1seg, :]\n",
    "df_pred_2seg = df_pred.iloc[list_2seg, :]\n",
    "df_pred_3seg = df_pred.iloc[list_3seg, :]\n",
    "df_pred_4seg = df_pred.iloc[list_4seg, :]\n",
    "df_pred_5seg = df_pred.iloc[list_5seg, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X24x8cw2IK5P",
    "outputId": "5366ab22-f4f8-4841-f2a2-8331edec9cd4"
   },
   "outputs": [],
   "source": [
    "print(\"Segundo 1: [{}, {}]\".format(min(df_pred_1seg['distance(m)']),\n",
    "                                   max(df_pred_1seg['distance(m)'])))\n",
    "print(\"Segundo 2: [{}, {}]\".format(min(df_pred_2seg['distance(m)']),\n",
    "                                   max(df_pred_2seg['distance(m)'])))\n",
    "print(\"Segundo 3: [{}, {}]\".format(min(df_pred_3seg['distance(m)']),\n",
    "                                   max(df_pred_3seg['distance(m)'])))\n",
    "print(\"Segundo 4: [{}, {}]\".format(min(df_pred_4seg['distance(m)']),\n",
    "                                   max(df_pred_4seg['distance(m)'])))\n",
    "print(\"Segundo 5: [{}, {}]\".format(min(df_pred_5seg['distance(m)']),\n",
    "                                   max(df_pred_5seg['distance(m)'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "ggBVRm3EIK5Q",
    "outputId": "32ef2403-c60f-45df-f0a4-65eed47d503d"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(25,4))\n",
    "df_pred_1seg.hist('distance(m)', ax=ax[0])\n",
    "df_pred_2seg.hist('distance(m)', ax=ax[1])\n",
    "df_pred_3seg.hist('distance(m)', ax=ax[2])\n",
    "df_pred_4seg.hist('distance(m)', ax=ax[3])\n",
    "df_pred_5seg.hist('distance(m)', ax=ax[4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "EYsQXJ9AIK5Q",
    "outputId": "c7c11640-8095-42c1-9051-c1a0ebcbba83"
   },
   "outputs": [],
   "source": [
    "df_pred_1seg.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "2KjnBXahIK5R",
    "outputId": "e7cdcd7d-b599-4b52-8c34-8dd3684ef422"
   },
   "outputs": [],
   "source": [
    "df_pred_2seg.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "q2Hs2XUvIK5R",
    "outputId": "6fc65113-784b-4adb-b8e2-744704feba7e"
   },
   "outputs": [],
   "source": [
    "df_pred_3seg.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "m_3QblQbIK5R",
    "outputId": "7ffe567d-e4ac-48e0-afea-2294819cf4e8"
   },
   "outputs": [],
   "source": [
    "df_pred_4seg.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "NGCqdTzEIK5S",
    "outputId": "5059b65f-1f11-4b9e-b528-d6e30c4ed739"
   },
   "outputs": [],
   "source": [
    "df_pred_5seg.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ydEiHFeIK5S"
   },
   "source": [
    "#### 2.1.1.3. Métricas del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTrFk4iUIK5M",
    "outputId": "282b0c78-5c78-4b2e-db6e-740b21b109aa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load fitted model \n",
    "MODEL_FOLDER = os.path.join(MODELS_FOLDER, \"simple\", \"UKF_PINN_LSTM_32units\")\n",
    "MODEL_FILE = os.path.join(MODEL_FOLDER, \"model_saved.hdf5\")\n",
    "model = tf.keras.models.load_model(MODEL_FILE, compile=False) #https://stackoverflow.com/questions/48373845/loading-model-with-custom-loss-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ikbGwcMJIK5S",
    "outputId": "f00fb559-5f4c-42d0-d4d7-721123c43051",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get test metrics\n",
    "METRICS_FILE = os.path.join(MODEL_FOLDER, \"metrics.txt\")\n",
    "get_metrics(model, X_test, df, vehicle_ids, df_col, df_nocol, df_dim, METRICS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from filterpy.kalman import UnscentedKalmanFilter, MerweScaledSigmaPoints\n",
    "# from filterpy.common import Q_discrete_white_noise\n",
    "# \n",
    "# def UKF(X_test):\n",
    "# \n",
    "#     y_pred = model.predict(X_test, verbose=0)\n",
    "# \n",
    "#     # Define UKF process and measurement functions\n",
    "#     def process_model(x, dt):\n",
    "#         # Simple kinematic model: x' = x + v * dt (transformado metros a lat lon (grados))\n",
    "#         x[0] += ((x[3] * np.sin(x[2]) * dt)/6371000)*(180/math.pi)\n",
    "#         x[1] += ((x[3] * np.cos(x[2]) * dt)/6371000)*(180/math.pi) / math.cos(x[0] * math.pi/180)\n",
    "# \n",
    "#         return x\n",
    "# \n",
    "#     def measurement_model(x):\n",
    "#         return np.array([x[0], x[1]])\n",
    "# \n",
    "#     y_test_UKF = []\n",
    "# \n",
    "#     for y in y_pred:\n",
    "# \n",
    "#         X_initial = [x[0:4] for x in y]\n",
    "# \n",
    "#         # Initialize UKF\n",
    "#         initial_state = X_initial[0] # Initial state\n",
    "#         ukf = UnscentedKalmanFilter(dim_x=4, dim_z=2, dt=1.0, fx=process_model, hx=measurement_model,\n",
    "#                                     points=MerweScaledSigmaPoints(4, alpha=0.1, beta=2., kappa=1.))\n",
    "#         ukf.x = initial_state\n",
    "#         ukf.P *= 0.1  # Initial uncertainty\n",
    "#         ukf.R *= 10  # Measurement noise\n",
    "#         ukf.Q = Q_discrete_white_noise(dim=4, dt=1.0, var=0.01)  # Process noise\n",
    "# \n",
    "#         # Initialize arrays to store estimated states\n",
    "#         estimated_states = np.zeros((len(X_initial), 4))\n",
    "# \n",
    "#         # Perform UKF prediction and update\n",
    "#         for i in range(len(X_initial)):\n",
    "#             #print(np.array([[x[0] for x in X_initial][i], [x[1] for x in X_initial][i]]))\n",
    "#             ukf.predict()\n",
    "#             ukf.update(np.array([[x[0] for x in X_initial][i], [x[1] for x in X_initial][i]]))\n",
    "#             estimated_states[i] = ukf.x\n",
    "# \n",
    "#         y_test_UKF.append(np.array(estimated_states))\n",
    "# \n",
    "#     y_test_UKF = np.array(y_test_UKF)\n",
    "#     return y_test_UKF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "eIZMo86MIBrS",
    "pYoL6psHIWzY",
    "svKg0PfOIK5M",
    "3lYfgH92IK5N",
    "_ydEiHFeIK5S",
    "-SJJ9NS9UdrS",
    "hGLm5oR-UdrU",
    "RvvtvAY-UdrX",
    "CRrqZHSDUdra",
    "XdtWyyYZyzEI",
    "CcyuOjzgyzEK",
    "VVXe6caHyzEN",
    "w4EPs2CmyzES",
    "U4lTd1agtSY_",
    "bmM_6dI5tSZB",
    "N88oAmqgtSZE",
    "bG64xgnetSZI",
    "AmqNpPBhXUOJ",
    "zIAPQDiiXUOK",
    "luPIrkImzW2N",
    "hwLs3QEmzW2P",
    "KObO0jqEzW2R",
    "i_JQN-4EzW2W",
    "KW1RMQdIgRgI",
    "ROIpg4X3Kbqh",
    "5UzAAgjyKbqi",
    "w_6Fv1gjKbqm",
    "3zrtDfgoKbqq",
    "oUczStJ17gMQ",
    "xGTN1DT47gMU",
    "TKyKY5Au7gMY",
    "R1OMfy_17gMd",
    "si03Z0QEtEDt",
    "UtJCmd9ptEDt",
    "-AW4gN1jtEDv",
    "1TChDWhbtEDy",
    "2FKP4G7oe_tI",
    "HSh-Wteae_tJ",
    "9o9rzDxte_tK",
    "BXOOoGNme_tM",
    "FDmWj6Q6tueF",
    "xztx0n7ZtueF",
    "tPeX0p_vtueG",
    "1NqIXlGMtueJ",
    "7SHTFnfrgaEn",
    "R3wf_s1EgaEo",
    "6a1sMdU1gaEp",
    "Q-FulacAgaEs",
    "pZSzqhTwI-K1",
    "71_C6qdTdgki",
    "24aLko9idgkj",
    "fSvHYhxsdgkl",
    "NyXYkxUbdgkp",
    "fvOZNM3WiDYv",
    "Wf4tPOn2iDYv",
    "NfZA3NhuiDYw",
    "fpADqT34iDYy",
    "vhjh8EXRJFzU",
    "wFoJzSUMf_zh",
    "i74V8xpqf_zh",
    "Rk5dYrnEf_zj",
    "LHRSoM2Df_zl",
    "N0EEJm-oitD5",
    "87iRiUrpitD5",
    "ZrM30hC0itD7",
    "uaPVid4ditEA",
    "spjfdXD5JKDe",
    "WkK2QczyxjD6",
    "rXnkN8JxxjD7",
    "a_xMeY4-xjD_",
    "qy1gzet4xjEJ",
    "I9Sz14gju7kY",
    "nV1WYQ-Fu7kY",
    "wX35qbaau7ka",
    "IyGeyoVBu7ke",
    "S0h-auEEKj-H",
    "K2JLpjjjKj-I",
    "YtPAuo03Kj-K",
    "BLGynV7yKj-P",
    "dTzCt1vQ37Yu",
    "Fkol9ABH37Yx",
    "B_iCbl_M37Y4",
    "tq3VYkYR37ZB"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
